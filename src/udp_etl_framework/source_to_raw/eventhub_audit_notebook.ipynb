{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df1785b0-a81f-4a33-ac00-ae80bd6da550",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Setting up the environment and loading the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd5b5281-f75f-4b55-93fe-dd49c6f051aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### a. Importing the libraries and configrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e58b9775-3990-48a8-a23d-7a11862a2c67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing required files & libraries\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import logging\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5df48ae4-8de2-4134-9c52-530cca4ac0ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "vietnam_tz = timezone(timedelta(hours=7))\n",
    "logging.Formatter.converter = lambda *args: datetime.now(vietnam_tz).timetuple()\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "915b1220-705f-4272-a9a2-9050263e225d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### b. Defining widgets and parameters & loading configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d023aefd-ce18-44b9-a02e-6ceb064ea41e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Defining widgets\n",
    "dbutils.widgets.text(\"environment\", \"\", \"\")\n",
    "dbutils.widgets.text(\"job_name\", \"\", \"\")\n",
    "# Fetching widgets values\n",
    "environment = dbutils.widgets.get(\"environment\")\n",
    "job_name = dbutils.widgets.get(\"job_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f11826fd-7b92-4fd4-bf83-169c293d1ae7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config = open(\"../configs/config.json\")\n",
    "settings = json.load(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "114db7bb-6de6-4126-add1-6d1725efb3fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog_name = settings[environment]['catalogName']\n",
    "bronze_schema = settings[environment]['bronzeSchema']\n",
    "eventhub_invalid_records_path= settings[environment][\"eventhub_invalid_records_path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "becbd471-5600-4e00-9a8f-02c291372cba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger.info(\n",
    "    \"\\n\".join(\n",
    "        [\n",
    "            \"Configuration Loaded:\",\n",
    "            f\"  catalog_name                        : {catalog_name}\",\n",
    "            f\"  bronze_schema                       : {bronze_schema}\",\n",
    "            f\"  eventhub_invalid_records_path       : {eventhub_invalid_records_path}\",\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a83384b6-0e32-4d20-bd6d-fc240ecd59f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2. Reading batch wise row count from eventhub audit log table,Aggregating it and writing it to consolidate ingestion adit log table with schema validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eadaed39-c850-4614-b8f2-026becc91665",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_eventhub_list(catalog_name: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Fetch list of raw table names (Event Hubs) from lookup table for a given namespace.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "        SELECT distinct table_name\n",
    "        FROM {catalog_name}.default.lookup_table_source2raw\n",
    "        WHERE source_name = 'EVENTHUB'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = spark.sql(query)\n",
    "        event_hubs_list = [\n",
    "            row[\"table_name\"] for row in df.select(\"table_name\").collect()\n",
    "        ]\n",
    "        logger.info(f\"Fetched Event Hub tables: {event_hubs_list}\")\n",
    "        return event_hubs_list\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to fetch Event Hub list: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def get_invalid_record_summary(\n",
    "    hubs: List[str],\n",
    "    invalid_path: str,\n",
    "    proc_date: str\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Returns schema validation summary for each Event Hub.\n",
    "    Calculates number of invalid records for a given date,\n",
    "    and identifies if schema mismatches occurred with corresponding error messages.\n",
    "    \"\"\"\n",
    "    audit_rows = []\n",
    "    for hub in hubs:\n",
    "        logger.info(f\"Checking hub: {hub}\")\n",
    "        try:\n",
    "            df = spark.read.format(\"delta\").load(f\"{invalid_path}/{hub}/\")\n",
    "            df = df.withColumn(\"ingestion_time\", to_date(col(\"ingestion_time\"), \"yyyyMMdd\"))\n",
    "            filtered_df = df.filter(col(\"ingestion_time\") == lit(proc_date))\n",
    "            invalid_count = filtered_df.count()\n",
    "            status = \"Mismatched\" if invalid_count > 0 else \"Matched\"\n",
    "            error_str = (\n",
    "                filtered_df.agg(concat_ws(\", \", collect_set(\"error\")).alias(\"error_string\")).collect()[0][\"error_string\"]\n",
    "                if invalid_count > 0 else None\n",
    "            )\n",
    "            audit_rows.append((hub, datetime.now(timezone(timedelta(hours=7))), invalid_count, status, error_str))\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading data for hub '{hub}': {str(e)}\")\n",
    "            audit_rows.append((hub, datetime.now(timezone(timedelta(hours=7))), -1, \"ERROR\", str(e)))\n",
    "\n",
    "    audit_schema = StructType([\n",
    "        StructField(\"hub\", StringType(), True),\n",
    "        StructField(\"audit_time_utc\", TimestampType(), True),\n",
    "        StructField(\"invalid_record_count\", IntegerType(), True),\n",
    "        StructField(\"schema_match_status\", StringType(), True),\n",
    "        StructField(\"schema_mismatches\", StringType(), True)\n",
    "    ])\n",
    "    return spark.createDataFrame(audit_rows, audit_schema)\n",
    "\n",
    "\n",
    "def get_bronze_summary(proc_date: str, catalog_name: str, hubs: List[str]) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Returns Bronze layer summary for a given proc_date.\n",
    "    Aggregates row count from the audit log per hub for the specified processing date.\n",
    "    \"\"\"\n",
    "    bronze_audit_df = spark.table(f\"{catalog_name}.default.eventhub_audit_log\")\n",
    "    actual_summary = (\n",
    "        bronze_audit_df.filter(col(\"proc_date\") == proc_date)\n",
    "        .select(\"proc_date\", \"row_count\", \"hub\")\n",
    "        .groupBy(\"hub\", \"proc_date\")\n",
    "        .agg(sum(\"row_count\").alias(\"streaming_df_count\"))\n",
    "    )\n",
    "\n",
    "    # Add 0-count rows for hubs that didn't appear\n",
    "    missing_hubs = set(hubs) - set([row[\"hub\"] for row in actual_summary.select(\"hub\").distinct().collect()])\n",
    "    missing_df = spark.createDataFrame([(hub, proc_date, 0) for hub in missing_hubs], actual_summary.schema)\n",
    "    return actual_summary.unionByName(missing_df)\n",
    "\n",
    "\n",
    "def get_delta_summary(hubs: List[str], proc_date: str, catalog_name: str, bronze_schema: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Returns Delta table summary for each Event Hub.\n",
    "    Aggregates row count per hub and date from Delta tables matching the proc_date.\n",
    "    \"\"\"\n",
    "    all_summaries = []\n",
    "    for hub in hubs:\n",
    "        try:\n",
    "            logger.info(f\"Processing delta table for hub: {hub}\")\n",
    "            delta_df = spark.table(f\"{catalog_name}.{bronze_schema}.{hub}\")\n",
    "            delta_df = delta_df.withColumn(\"proc_date\", col(\"ProcDate\").substr(1, 8))\n",
    "            filtered_df = delta_df.filter(col(\"proc_date\") == proc_date)\n",
    "            summary = (\n",
    "                filtered_df.groupBy(\"proc_date\")\n",
    "                .agg(count(\"*\").alias(\"delta_row_count\"))\n",
    "                .withColumn(\"hub\", lit(hub))\n",
    "            )\n",
    "            all_summaries.append(summary)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing hub {hub}: {str(e)}\")\n",
    "    if not all_summaries:\n",
    "        raise ValueError(\"No delta summaries were generated.\")\n",
    "    df = all_summaries[0]\n",
    "    for other_df in all_summaries[1:]:\n",
    "        df = df.unionByName(other_df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_final_audit_df(\n",
    "    bronze_df: DataFrame,\n",
    "    delta_df: DataFrame,\n",
    "    audit_df: DataFrame\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Combines Bronze, Delta, and Invalid record summaries into a single audit DataFrame\n",
    "    with metrics: streaming row count, delta row count, invalid record count, and schema status.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        bronze_df.alias(\"bronze\")\n",
    "        .join(delta_df.alias(\"delta\"), [\"hub\", \"proc_date\"], \"outer\")\n",
    "        .join(audit_df.alias(\"audit\"), \"hub\", \"left\")\n",
    "        .select(\n",
    "            \"bronze.hub\", \"bronze.proc_date\",\n",
    "            \"bronze.streaming_df_count\", \"delta.delta_row_count\",\n",
    "            col(\"audit.invalid_record_count\"),\n",
    "            col(\"audit.schema_match_status\"),\n",
    "            col(\"audit.schema_mismatches\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def write_to_audit_table(\n",
    "    final_df: DataFrame,\n",
    "    catalog_name: str,\n",
    "    proc_date_col: str = \"proc_date\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Writes the final audit DataFrame to the consolidated audit result Delta table\n",
    "    after enriching with metadata fields like source info, load date, and pipeline name.\n",
    "    \"\"\"\n",
    "    target_schema = spark.table(f\"{catalog_name}.default.consolidate_audit_result\").schema\n",
    "\n",
    "    enriched_df = final_df \\\n",
    "        .withColumn(\"load_date\", to_date(col(proc_date_col), \"yyyyMMdd\")) \\\n",
    "        .withColumn(\"source_name\", lit(\"EventHub\")) \\\n",
    "        .withColumn(\"source_description\", lit(\"EventHub\")) \\\n",
    "        .withColumn(\"table_name\", col(\"hub\")) \\\n",
    "        .withColumn(\"pipeline_name\", lit(\"eventhub_source_to_bronze_workflow\")) \\\n",
    "        .withColumn(\"environment\", lit(\"dev\")) \\\n",
    "        .withColumn(\"source_object_count\", col(\"streaming_df_count\").cast(\"string\")) \\\n",
    "        .withColumn(\"target_object_count\", col(\"delta_row_count\").cast(\"string\"))\n",
    "\n",
    "    for field in target_schema:\n",
    "        if field.name not in enriched_df.columns:\n",
    "            enriched_df = enriched_df.withColumn(field.name, lit(None).cast(field.dataType))\n",
    "        else:\n",
    "            enriched_df = enriched_df.withColumn(field.name, col(field.name).cast(field.dataType))\n",
    "\n",
    "    final_insert_df = enriched_df.select(*[field.name for field in target_schema])\n",
    "    final_insert_df.write.mode(\"append\").format(\"delta\").saveAsTable(f\"{catalog_name}.default.consolidate_audit_result\")\n",
    "    return final_insert_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d79721a-e880-481b-af1c-62c73ff82de4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def main(\n",
    "    hubs: List[str],\n",
    "    invalid_path: str,\n",
    "    proc_date: str,\n",
    "    catalog_name: str,\n",
    "    bronze_schema: str\n",
    "):\n",
    "    logger.info(f\"Starting EventHub audit process for proc_date: {proc_date}\")\n",
    "\n",
    "    # Step 1: Get invalid record counts and schema mismatch status\n",
    "    audit_df = get_invalid_record_summary(hubs, invalid_path, proc_date)\n",
    "    logger.info(\"Completed schema validation audit summary.\")\n",
    "\n",
    "    # Step 2: Get Bronze summary\n",
    "    bronze_df = get_bronze_summary(proc_date,catalog_name,hubs)\n",
    "    logger.info(\"Fetched Bronze layer summary.\")\n",
    "\n",
    "    # Step 3: Get Delta summary\n",
    "    delta_df = get_delta_summary(hubs, proc_date,catalog_name,bronze_schema)\n",
    "    logger.info(\"Fetched Delta layer summary.\")\n",
    "\n",
    "    # Step 4: Join summaries to build final audit DataFrame\n",
    "    final_df = build_final_audit_df(bronze_df, delta_df, audit_df)\n",
    "    logger.info(\"Built final audit DataFrame.\")\n",
    "\n",
    "    # Step 5: Write final DataFrame to audit table\n",
    "    write_to_audit_table(final_df, catalog_name)\n",
    "    logger.info(\"Final audit data written to target table successfully.\")\n",
    "\n",
    "# Fetch Event Hub metadata\n",
    "event_hubs_list = get_eventhub_list(catalog_name)\n",
    "if not event_hubs_list:\n",
    "    raise ValueError(\"No Event Hub tables found\")\n",
    "\n",
    "yesterday = (datetime.now(timezone(timedelta(hours=7))) - timedelta(days=1)).strftime(\"%Y%m%d\")\n",
    "\n",
    "main(\n",
    "    hubs=event_hubs_list,  \n",
    "    invalid_path=eventhub_invalid_records_path,  \n",
    "    proc_date=yesterday,\n",
    "    catalog_name=catalog_name,\n",
    "    bronze_schema=bronze_schema\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3437076432833682,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "eventhub_audit_notebook",
   "widgets": {
    "environment": {
     "currentValue": "DEV",
     "nuid": "3eecf6a6-35ce-407e-8bea-2c1311472250",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "environment",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "environment",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "eventhub_namespace": {
     "currentValue": "ehns-udp-datawarehouse-prod-sea-001",
     "nuid": "85588f46-ea55-428c-b349-d8e6499c5d3e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "eventhub_namespace",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "eventhub_namespace",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "job_name": {
     "currentValue": "eventhub_consolidated_audit_logs_load",
     "nuid": "59b08e6c-4a8d-4347-be05-4ce7343eb632",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "job_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "job_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
