{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df1785b0-a81f-4a33-ac00-ae80bd6da550",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Setting up the environment and loading the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd5b5281-f75f-4b55-93fe-dd49c6f051aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### a. Importing the libraries and configrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "396e8be2-ff2a-4841-a09f-18b66fd0d01f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing required files & libraries\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import logging\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import schemas  \n",
    "from schemas import JsonSchemaValidator\n",
    "import pandas as pd\n",
    "from pyspark.sql.streaming import StreamingQuery\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d9d55d7-b668-4a20-8c91-9acc4c6935f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "vietnam_tz = timezone(timedelta(hours=7))\n",
    "logging.Formatter.converter = lambda *args: datetime.now(vietnam_tz).timetuple()\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "915b1220-705f-4272-a9a2-9050263e225d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### b. Defining widgets and parameters & loading configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d023aefd-ce18-44b9-a02e-6ceb064ea41e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Defining widgets\n",
    "dbutils.widgets.text(\"environment\", \"\", \"\")\n",
    "dbutils.widgets.text(\"job_name\", \"\", \"\")\n",
    "dbutils.widgets.text(\"eventhub_namespace\", \"\", \"\")\n",
    "# Fetching widgets values\n",
    "environment = dbutils.widgets.get(\"environment\")\n",
    "job_name = dbutils.widgets.get(\"job_name\")\n",
    "eventhub_namespace = dbutils.widgets.get(\"eventhub_namespace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f11826fd-7b92-4fd4-bf83-169c293d1ae7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config = open(\"../configs/config.json\")\n",
    "settings = json.load(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "114db7bb-6de6-4126-add1-6d1725efb3fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog_name = settings[environment]['catalogName']\n",
    "bronze_schema = settings[environment]['bronzeSchema']\n",
    "bootstrap_servers = settings[environment][f\"eventhubs_serverDetails_{eventhub_namespace}\"]\n",
    "scope_name = settings[environment][f\"scope_name\"]\n",
    "secret_key_name = settings[environment][f\"secret_key_name_{eventhub_namespace}\"]\n",
    "eventhubs_connection_string = dbutils.secrets.get(scope=scope_name, key=secret_key_name)\n",
    "eventhub_raw_path = settings[environment][\"eventhub_raw_path\"]\n",
    "eventhub_raw_checkpoint = settings[environment][\"eventhub_raw_checkpoint\"]\n",
    "eventhub_bronze_schema_location = settings[environment][\"eventhub_bronze_schema_location\"]\n",
    "eventhub_bronze_checkpoint = settings[environment][\"eventhub_bronze_checkpoint\"]\n",
    "eventhub_invalid_records_checkpoint= settings[environment][\"eventhub_invalid_records_checkpoint\"]\n",
    "eventhub_invalid_records_path= settings[environment][\"eventhub_invalid_records_path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "becbd471-5600-4e00-9a8f-02c291372cba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger.info(\n",
    "    \"\\n\".join(\n",
    "        [\n",
    "            \"Configuration Loaded:\",\n",
    "            f\"  catalog_name                        : {catalog_name}\",\n",
    "            f\"  bronze_schema                       : {bronze_schema}\",\n",
    "            f\"  bootstrap_servers                   : {bootstrap_servers}\",\n",
    "            f\"  eventhubs_connection_string         : {eventhubs_connection_string}\",\n",
    "            f\"  eventhub_raw_path                   : {eventhub_raw_path}\",\n",
    "            f\"  eventhub_raw_checkpoint             : {eventhub_raw_checkpoint}\",\n",
    "            f\"  eventhub_bronze_schema_location     : {eventhub_bronze_schema_location}\",\n",
    "            f\"  eventhub_bronze_checkpoint          : {eventhub_bronze_checkpoint}\",\n",
    "            f\"  eventhub_invalid_records_checkpoint : {eventhub_invalid_records_checkpoint}\",\n",
    "            f\"  eventhub_invalid_records_path       : {eventhub_invalid_records_path}\",\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23e9c4b2-432e-4455-a405-38fd713bffa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### c. Setting Up connection string for EventHub conection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f1fc4ce-b5c2-4658-8d9f-746a121699a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eh_sasl = f'kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"$ConnectionString\\\"password=\\\"{eventhubs_connection_string}\";'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a83384b6-0e32-4d20-bd6d-fc240ecd59f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2. Reading data from the EventHub and writing it to bronze layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40bed93d-9d74-4be1-a4f4-363d07d2eb5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------- Define Output Schema for Validation Results ----------\n",
    "validate_schema = StructType(\n",
    "    [\n",
    "        StructField(\"is_valid\", BooleanType(), True),\n",
    "        StructField(\"error\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# ---------- Pandas UDF for Schema Validation ----------\n",
    "def create_validation_udf(validators_dict: Dict[str, any]) -> Callable[[pd.Series, pd.Series], pd.DataFrame]:\n",
    "    try:\n",
    "        @pandas_udf(validate_schema)\n",
    "        def validate_all(json_series: pd.Series, hub_series: pd.Series) -> pd.DataFrame:\n",
    "            is_valid_list, error_list = [], []\n",
    "            for json_str, hub in zip(json_series, hub_series):\n",
    "                try:\n",
    "                    obj = json.loads(json_str)\n",
    "                    validators_dict[hub].validate_instance(obj)\n",
    "                    is_valid_list.append(True)\n",
    "                    error_list.append(None)\n",
    "                except Exception as e:\n",
    "                    is_valid_list.append(False)\n",
    "                    error_list.append(str(e))\n",
    "            return pd.DataFrame({\"is_valid\": is_valid_list, \"error\": error_list})\n",
    "\n",
    "        return validate_all\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create validation UDF: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# ---------- Functions ----------\n",
    "\n",
    "\n",
    "def get_eventhub_list(catalog_name: str, eventhub_namespace: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Fetch list of raw table names (Event Hubs) from lookup table for a given namespace.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "        SELECT distinct table_name\n",
    "        FROM {catalog_name}.default.lookup_table_source2raw\n",
    "        WHERE source_name = 'EVENTHUB'\n",
    "          AND eventhub_namespace = '{eventhub_namespace}'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = spark.sql(query)\n",
    "        event_hubs_list = [\n",
    "            row[\"table_name\"] for row in df.select(\"table_name\").collect()\n",
    "        ]\n",
    "        logger.info(\n",
    "            f\"Fetched Event Hub tables for eventhub namespace '{eventhub_namespace}': {event_hubs_list}\"\n",
    "        )\n",
    "        return event_hubs_list\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to fetch Event Hub list for namespace {eventhub_namespace}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def get_validators(event_hubs_list: List[str]) -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Dynamically fetch validator objects from the schemas module for each hub.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        validators = {\n",
    "            hub: getattr(schemas, hub)\n",
    "            for hub in event_hubs_list\n",
    "            if hasattr(schemas, hub)  # Only include if the schema is defined\n",
    "        }\n",
    "        logger.info(f\"Validators loaded for hubs: {list(validators.keys())}\")\n",
    "        return validators\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to build validators from schemas module: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def read_eventhub_stream(hubs: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a Kafka-based Spark readStream using the given comma-separated list of topics (Event Hubs).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return (\n",
    "            spark.readStream.format(\"kafka\")\n",
    "            .option(\"subscribe\", hubs)\n",
    "            .option(\"kafka.bootstrap.servers\", bootstrap_servers)\n",
    "            .option(\"startingOffsets\", \"earliest\")\n",
    "            .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "            .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "            .option(\"kafka.sasl.jaas.config\", eh_sasl)\n",
    "            .option(\"failOnDataLoss\", \"false\")\n",
    "            .option(\"kafka.request.timeout.ms\", \"60000\")\n",
    "            .option(\"kafka.session.timeout.ms\", \"30000\")\n",
    "            .load()\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to start Event Hub stream for hubs '{hubs}': {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def decode_stream_data(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Decodes Kafka messages to readable JSON string and extracts topic name as `hub`.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return df.select(\n",
    "            col(\"topic\").alias(\"hub\"),\n",
    "            decode(col(\"value\"), \"UTF-8\").cast(\"string\").alias(\"json_str\"),\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to decode Kafka stream data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def write_valid_invalid_streams(\n",
    "    hub: str,\n",
    "    decoded_df: DataFrame,\n",
    "    validate_all: Callable[[pd.Series, pd.Series], pd.DataFrame],\n",
    "    stream_queries: List[Tuple[str, StreamingQuery]]\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    For a given hub, splits valid and invalid records and writes them to raw and invalid paths respectively.\n",
    "    Appends the active stream queries to `stream_queries` for monitoring.\n",
    "    \"\"\"\n",
    "    # Apply schema validation using UDF\n",
    "    validated_df = (\n",
    "        decoded_df.withColumn(\"validation\", validate_all(col(\"json_str\"), col(\"hub\")))\n",
    "        .withColumn(\"is_valid\", col(\"validation.is_valid\"))\n",
    "        .withColumn(\"error\", col(\"validation.error\"))\n",
    "        .drop(\"validation\")\n",
    "    )\n",
    "\n",
    "    # Filter only records from this hub\n",
    "    filtered_df = validated_df.filter(col(\"hub\") == hub)\n",
    "\n",
    "    # Separate valid records\n",
    "    valid_df = filtered_df.filter(col(\"is_valid\") == True).drop(\n",
    "        \"is_valid\", \"error\", \"hub\"\n",
    "    )\n",
    "\n",
    "    # Add timestamp to invalid records\n",
    "    invalid_df = (\n",
    "        filtered_df.filter(col(\"is_valid\") == False)\n",
    "        .withColumn(\n",
    "            \"ingestion_time\",\n",
    "            from_utc_timestamp(current_timestamp(), \"Asia/Ho_Chi_Minh\"),\n",
    "        )\n",
    "        .drop(\"is_valid\", \"hub\")\n",
    "    )\n",
    "\n",
    "    # Write valid records to raw path (as text)\n",
    "    try:\n",
    "        write_valid = (\n",
    "            valid_df.writeStream.format(\"text\")\n",
    "            .option(\"checkpointLocation\", f\"{eventhub_raw_checkpoint}/{hub}\")\n",
    "            .option(\"path\", f\"{eventhub_raw_path}/{hub}\")\n",
    "            .outputMode(\"append\")\n",
    "            .trigger(processingTime=\"15 minutes\")\n",
    "            .start()\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to start write stream for valid records of {hub}: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Write invalid records to Delta\n",
    "    try:\n",
    "        write_invalid = (\n",
    "        invalid_df.writeStream.format(\"delta\")\n",
    "        .option(\"checkpointLocation\", f\"{eventhub_invalid_records_checkpoint}/{hub}\")\n",
    "        .option(\"path\", f\"{eventhub_invalid_records_path}/{hub}\")\n",
    "        .outputMode(\"append\")\n",
    "        .trigger(processingTime=\"15 minutes\")\n",
    "        .start()\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to start write stream for invalid records of {hub}: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Store queries for later monitoring\n",
    "    stream_queries.extend(\n",
    "        [(f\"{hub}_raw_stream\", write_valid), (f\"{hub}_invalid_stream\", write_invalid)]\n",
    "    )\n",
    "\n",
    "\n",
    "def start_bronze_stream_for_hub(\n",
    "    hub: str,\n",
    "    catalog_name: str,\n",
    "    bronze_schema: str,\n",
    "    stream_queries: List[Tuple[str, StreamingQuery]]\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Reads raw data from Event Hub text files and writes to a Delta table in Bronze layer.\n",
    "    Handles dynamic or missing `ProcDate` field.\n",
    "    \"\"\"\n",
    "    df = (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"json\")\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "        .option(\"cloudFiles.schemaLocation\", f\"{eventhub_bronze_schema_location}/{hub}\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .load(f\"{eventhub_raw_path}/{hub}\")\n",
    "    )\n",
    "\n",
    "    # Normalize or add ProcDate\n",
    "    if \"ProcDate\" not in df.columns:\n",
    "        df = df.withColumn(\n",
    "            \"ProcDate\",\n",
    "            date_format(\n",
    "                from_utc_timestamp(current_timestamp(), \"Asia/Ho_Chi_Minh\"),\n",
    "                \"yyyyMMddHHmmss\",\n",
    "            ),\n",
    "        )\n",
    "    else:\n",
    "        df = df.withColumn(\n",
    "            \"ProcDate\",\n",
    "            when(col(\"ProcDate\").rlike(\"^[0-9]{14}$\"), col(\"ProcDate\")).otherwise(\n",
    "                date_format(\n",
    "                    to_timestamp(col(\"ProcDate\"), \"yyyy-MM-dd HH:mm:ss\"),\n",
    "                    \"yyyyMMddHHmmss\",\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "    df=df.withColumn(\n",
    "                \"source_metadata\",\n",
    "                concat_ws(\n",
    "                    \"|\",\n",
    "                    col(\"_metadata.file_path\"),\n",
    "                    from_utc_timestamp(col(\"_metadata.file_modification_time\"), \"Asia/Ho_Chi_Minh\").cast(\"string\")\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def write_batch_with_rowcount_audit(batch_df, batch_id):\n",
    "        try:\n",
    "            if batch_df.count()>0:\n",
    "                batch_df.write.format(\"delta\").mode(\"append\").partitionBy(\"ProcDate\").option(\"mergeSchema\", \"true\").saveAsTable(f\"{catalog_name}.{bronze_schema}.{hub}\")\n",
    "                batch_ts = from_utc_timestamp(current_timestamp(), \"Asia/Ho_Chi_Minh\")\n",
    "                audit_df = (\n",
    "                    batch_df\n",
    "                    .withColumn(\"proc_date\", substring(\"ProcDate\", 1, 8))  # extract yyMMdd from yyyyMMddHHmmss\n",
    "                    .groupBy(\"proc_date\")\n",
    "                    .agg(\n",
    "                        count(\"*\").alias(\"row_count\")\n",
    "                    )\n",
    "                    .withColumn(\"hub\", lit(hub))\n",
    "                    .withColumn(\"batch_id\", lit(batch_id))\n",
    "                    .withColumn(\"batch_timestamp\", lit(batch_ts))\n",
    "                )\n",
    "\n",
    "                audit_df.write.format(\"delta\").mode(\"append\").saveAsTable(f\"{catalog_name}.default.eventhub_audit_log\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to write audit log for {hub}: {e}\")\n",
    "            raise\n",
    "\n",
    "    # Write to Bronze Delta table\n",
    "    try:\n",
    "        write_bronze = (\n",
    "            df.writeStream\n",
    "            .foreachBatch(write_batch_with_rowcount_audit)\n",
    "            .option(\"checkpointLocation\", f\"{eventhub_bronze_checkpoint}/{hub}\")\n",
    "            .trigger(processingTime=\"15 minutes\")\n",
    "            .start()\n",
    "        )\n",
    "        logger.info(f\"Bronze stream started for {hub}\")\n",
    "        write_bronze_marker_if_missing(hub)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to start write bronze stream of {hub}: {e}\")\n",
    "        raise\n",
    "    stream_queries.append((f\"{hub}_bronze_stream\", write_bronze))\n",
    "\n",
    "\n",
    "def write_bronze_marker_if_missing(hub: str) -> None:\n",
    "    \"\"\"\n",
    "    Writes a marker file named `_bronze_started` in the Event Hub's raw directory if it doesn't already exist.\n",
    "    This marker indicates that the Bronze stream has been initiated for the given hub.\n",
    "    \"\"\"\n",
    "    marker_path = f\"{eventhub_raw_path}/{hub}/_bronze_started\"\n",
    "    try:\n",
    "        if not any(f.name == \"_bronze_started\" for f in dbutils.fs.ls(f\"{eventhub_raw_path}/{hub}\")):\n",
    "            dbutils.fs.put(marker_path, \"\", overwrite=False)\n",
    "            logger.info(f\"Wrote bronze started marker for hub: {hub}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to write bronze marker for {hub}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def is_data_available(path: str) -> bool:\n",
    "    \"\"\"\n",
    "    Checks whether raw data exists or bronze already started for this hub.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if any(f.name == \"_bronze_started\" for f in dbutils.fs.ls(path)):\n",
    "            return True  # Skip expensive listing if already started\n",
    "        files = dbutils.fs.ls(path)\n",
    "        return any(\n",
    "            f.size > 0 and not f.name.startswith(\"_\") and not f.name.startswith(\".\")\n",
    "            for f in files\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to check files at path {path}: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def check_and_start_stream(hub: str) -> None:\n",
    "    \"\"\"\n",
    "    Keep checking until data is available for the hub.\n",
    "    Once data is found, trigger the Bronze stream.\n",
    "    \"\"\"\n",
    "    path = f\"{eventhub_raw_path}/{hub}\"\n",
    "    while True:\n",
    "        if is_data_available(path):\n",
    "            logger.info(f\"Data found for hub: {hub} â€” starting bronze stream\")\n",
    "            try:\n",
    "                start_bronze_stream_for_hub(hub, catalog_name, bronze_schema, stream_queries)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error starting bronze stream for {hub}: {e}\")\n",
    "            break  # Exit loop after starting stream\n",
    "        else:\n",
    "            logger.info(f\"Waiting for data for hub: {hub}. Retrying in 60s...\")\n",
    "            time.sleep(60)\n",
    "\n",
    "\n",
    "def monitor_streams(\n",
    "    queries: List[Tuple[str, StreamingQuery]]\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Monitors multiple Spark Streaming queries in parallel.\n",
    "\n",
    "    \"\"\"\n",
    "    def wait_for_query(name, query):\n",
    "        try:\n",
    "            query.awaitTermination()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Streaming query '{name}' failed: {e}\")\n",
    "            raise RuntimeError(f\"Query '{name}' terminated with error.\")\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(wait_for_query, name, query) for name, query in queries]\n",
    "        for f in futures:\n",
    "            f.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc03d4d7-5ac6-4d4e-b84b-a3ee02a4db34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------- Main Function ----------\n",
    "def main():\n",
    "    global stream_queries\n",
    "    stream_queries = [] # To track all active streaming queries for monitoring\n",
    "\n",
    "    # Step 1: Get Event Hub metadata and prepare validators\n",
    "    # Retrieve list of Event Hub table names from the lookup table\n",
    "    event_hubs_list = get_eventhub_list(catalog_name, eventhub_namespace)\n",
    "\n",
    "    # Checking if no Event Hubs are found for the given namespace\n",
    "    if not event_hubs_list:\n",
    "        logger.error(f\"No Event Hub tables found for namespace '{eventhub_namespace}'. Exiting.\")\n",
    "        raise ValueError(f\"No Event Hub tables found for namespace '{eventhub_namespace}'.\")\n",
    "\n",
    "    # Dynamically fetch JSON schema validators for each hub\n",
    "    validators = get_validators(event_hubs_list)\n",
    "\n",
    "    # Check for hubs without schemas\n",
    "    hubs_without_schema = [hub for hub in event_hubs_list if hub not in validators]\n",
    "    if hubs_without_schema:\n",
    "        logger.error(f\"The following hubs do not have schemas defined: {hubs_without_schema}\")\n",
    "        raise ValueError(f\"No schema defined for hubs: {hubs_without_schema}\")\n",
    "\n",
    "    # Create the validation UDF using the validator dictionary\n",
    "    validate_all = create_validation_udf(validators)\n",
    "    hubs = \",\".join(event_hubs_list)\n",
    "\n",
    "    # Step 2: Read Kafka stream and decode,Validate and Write valid and invalid data streams per hub\n",
    "    for hub in event_hubs_list:\n",
    "        logger.info(f\"Initialized read stream for the hub: {hub}\")\n",
    "        raw_eventhub_df = read_eventhub_stream(hub)\n",
    "        decoded_df = decode_stream_data(raw_eventhub_df)\n",
    "        logger.info(f\"Initialized write stream for the hub: {hub}\")\n",
    "        write_valid_invalid_streams(hub, decoded_df, validate_all, stream_queries)\n",
    "\n",
    "    # Step 3: trigger bronze streams in parallel when data is found\n",
    "    logger.info(\"Waiting for raw data to trigger bronze ingestion per hub...\")\n",
    "    with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "        executor.map(check_and_start_stream, event_hubs_list)\n",
    "    logger.info(\"All bronze streams triggered\")\n",
    "    \n",
    "    # Step 4: Monitor all stream queries continuously\n",
    "    monitor_streams(stream_queries)\n",
    "\n",
    "\n",
    "# ---------- Entry Point ----------\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Pipeline failed with unexpected error: {e}\")\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5423556739759036,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "eventhub_source_to_bronze",
   "widgets": {
    "environment": {
     "currentValue": "PROD",
     "nuid": "3eecf6a6-35ce-407e-8bea-2c1311472250",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "environment",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "environment",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "eventhub_namespace": {
     "currentValue": "ehns-udp-datawarehouse-prod-sea-001",
     "nuid": "85588f46-ea55-428c-b349-d8e6499c5d3e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "eventhub_namespace",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "eventhub_namespace",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "job_name": {
     "currentValue": "eventhub_raw_to_bronze_data_load",
     "nuid": "59b08e6c-4a8d-4347-be05-4ce7343eb632",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "job_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "job_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
