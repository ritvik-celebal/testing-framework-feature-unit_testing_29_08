{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ce297ab-7386-436c-a030-b28fb5453b81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"environment\", \"\")\n",
    "environment = dbutils.widgets.get(\"environment\")\n",
    "dbutils.widgets.text(\"catalog_name\", \"\")\n",
    "catalog_name = dbutils.widgets.get(\"catalog_name\")\n",
    "dbutils.widgets.text(\"schema_name\", \"\")\n",
    "schema_name = dbutils.widgets.get(\"schema_name\")\n",
    "dbutils.widgets.text(\"table_name\", \"\")\n",
    "table_name = dbutils.widgets.get(\"table_name\")\n",
    "\n",
    "dbutils.widgets.text(\"proc_date\", \"\")\n",
    "proc_date = dbutils.widgets.get(\"proc_date\")\n",
    "dbutils.widgets.text(\"timestamp\", \"\")\n",
    "timestamp = dbutils.widgets.get(\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "512496fe-f6bd-4b1c-bd10-2603af3a19e5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Updated_Monitoring_logs"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import MapType, ArrayType, StructType, IntegerType, LongType, StringType\n",
    "from pyspark.sql import Row, DataFrame\n",
    "from pyspark.sql.functions import col, when, count, approx_count_distinct, countDistinct, min, max, length, sum as spark_sum, cast\n",
    "from typing import List\n",
    "\n",
    "# Function to compute column-level metrics for a given DataFrame\n",
    "def compute_column_metrics(df: DataFrame, table_name: str, layer: str, timestamp, proc_date, operation, deleted, inserted, updated) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Computes column-level metrics for a given Spark DataFrame.\n",
    "\n",
    "    Supported Metrics:\n",
    "    ------------------\n",
    "    - Null count for all columns\n",
    "    - distinct count\n",
    "    - Min and Max values (IntegerType, LongType)\n",
    "    - Min, Max, and Total length (StringType)\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Input DataFrame to be profiled.\n",
    "    table_name : str\n",
    "        Name of the table.\n",
    "    layer : str\n",
    "        Data layer from schema name (e.g., bronze, silver, gold).\n",
    "    operation : str\n",
    "        Type of operation (WRITE, MERGE, DELETE, etc.).\n",
    "    deleted : int\n",
    "        Number of records deleted.\n",
    "    inserted : int\n",
    "        Number of records inserted.\n",
    "    updated : int\n",
    "        Number of records updated.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        A single-row DataFrame containing:\n",
    "        - table metadata\n",
    "        - array of column-level metric structs\n",
    "    \"\"\"\n",
    "\n",
    "    schema = df.schema\n",
    "    # simple_cols = [f.name for f in schema if not isinstance(f.dataType, (MapType, ArrayType, StructType))]\n",
    "    # Extract all column names \n",
    "    simple_cols = [f.name for f in schema]\n",
    "    # Skip if no columns found\n",
    "    if not simple_cols:\n",
    "        return None\n",
    "    \n",
    "    # Compute null counts for each column\n",
    "    null_exprs = [count(when(col(c).isNull(), 1)).alias(f\"{c}__nulls\") for c in simple_cols]\n",
    "\n",
    "    # Compute distinct count for each column\n",
    "    distinct_exprs = [countDistinct(col(c)).alias(f\"{c}__distinct\") for c in simple_cols]\n",
    "\n",
    "    # Initialize expressions for numeric min/max and string metrics\n",
    "    min_exprs = []\n",
    "    max_exprs = []\n",
    "    str_len_exprs = []\n",
    "    str_sum_len_exprs = []\n",
    "\n",
    "    # Loop over each column in schema to build additional expressions\n",
    "    \"\"\"\n",
    "    Loop through each column in the DataFrame schema to build type-specific aggregation expressions.\n",
    "\n",
    "    For each column in `simple_cols`:\n",
    "    - If the column is of IntegerType or LongType:\n",
    "        - Compute minimum and maximum values.\n",
    "    - If the column is of StringType:\n",
    "        - Compute minimum and maximum string lengths.\n",
    "        - Compute total string length across all rows.\n",
    "\n",
    "    All expressions are collected into `agg_exprs`, which is used to perform a aggregation\n",
    "    across the DataFrame. The result is collected as a Row object (`agg_result`).\n",
    "    \"\"\"\n",
    "    for field in schema:\n",
    "        if field.name not in simple_cols:\n",
    "            continue\n",
    "        col_name = field.name\n",
    "        dtype = field.dataType\n",
    "\n",
    "        # Add min and max expressions for integer/long columns\n",
    "        if isinstance(dtype, (IntegerType, LongType)):\n",
    "            min_exprs.append(min(col(col_name)).alias(f\"{col_name}__min\"))\n",
    "            max_exprs.append(max(col(col_name)).alias(f\"{col_name}__max\"))\n",
    "\n",
    "        # Add min, max, length expressions for string columns\n",
    "        elif isinstance(dtype, StringType):\n",
    "            str_len_exprs.append(min(length(col(col_name))).alias(f\"{col_name}__min_length\"))\n",
    "            str_len_exprs.append(max(length(col(col_name))).alias(f\"{col_name}__max_length\"))\n",
    "            str_sum_len_exprs.append(spark_sum(length(col(col_name))).alias(f\"{col_name}__total_length\"))\n",
    "\n",
    "    # Combine all aggregation expressions\n",
    "    agg_exprs = null_exprs + distinct_exprs + min_exprs + max_exprs + str_len_exprs + str_sum_len_exprs\n",
    "\n",
    "    # Perform aggregation on DataFrame and collect result\n",
    "    agg_result = df.agg(*agg_exprs).collect()[0]\n",
    "\n",
    "    # Build the column_metrics as an array of Row objects with all collected stats\n",
    "    \"\"\"\n",
    "    Define the `column_metrics` list as an array of Row objects.\n",
    "\n",
    "    For each column in `simple_cols`, generate:\n",
    "    - Null count and distinct count (all types)\n",
    "    - Min/max values for Integer/Long types\n",
    "    - Min/max/total string length for StringType\n",
    "\n",
    "    Each metric dictionary is converted to a Row and added to `column_metrics`.\n",
    "    \"\"\"\n",
    "    column_metrics = []\n",
    "    for field in schema:\n",
    "        if field.name not in simple_cols:\n",
    "            continue\n",
    "        c = field.name\n",
    "        dtype = field.dataType\n",
    "\n",
    "        # Initialize default metrics structure for the column\n",
    "        metrics = {\n",
    "            \"column_name\": c,\n",
    "            \"null_count\": agg_result[f\"{c}__nulls\"],\n",
    "            \"distinct_count\": agg_result[f\"{c}__distinct\"],\n",
    "            \"min_value\": None,\n",
    "            \"max_value\": None,\n",
    "            \"min_length\": None,\n",
    "            \"max_length\": None,\n",
    "            \"total_length\": None\n",
    "        }\n",
    "\n",
    "        # Populate numeric min,max for integer/long columns\n",
    "        if isinstance(dtype, (IntegerType, LongType)):\n",
    "            metrics[\"min_value\"] = agg_result[f\"{c}__min\"]\n",
    "            metrics[\"max_value\"] = agg_result[f\"{c}__max\"]\n",
    "\n",
    "        # Populate min, max, length metrics for string column\n",
    "        if isinstance(dtype, StringType):\n",
    "            metrics[\"min_length\"] = agg_result[f\"{c}__min_length\"]\n",
    "            metrics[\"max_length\"] = agg_result[f\"{c}__max_length\"]\n",
    "            metrics[\"total_length\"] = agg_result[f\"{c}__total_length\"]\n",
    "\n",
    "        column_metrics.append(Row(**metrics))\n",
    "\n",
    "    # Define the schema of the final output DataFrame\n",
    "    \"\"\"\n",
    "    Define the final output schema and structure the metrics results into a single-row DataFrame.\n",
    "\n",
    "    - `result_schema`: Specifies metadata and column-level metrics structure.\n",
    "    - `output_row`: A Row object capturing table-level and column-level metrics.\n",
    "    - Returns: A DataFrame containing one row for the current table.\n",
    "    \"\"\"\n",
    "    result_schema = \"\"\"\n",
    "        table_name string,\n",
    "        layer string,\n",
    "        timestamp string,\n",
    "        proc_date string,\n",
    "        operation string,\n",
    "        deleted_records long,\n",
    "        inserted_records long,\n",
    "        updated_records long,\n",
    "        error string,\n",
    "        status string,\n",
    "        column_metrics array<struct<\n",
    "            column_name: string,\n",
    "            null_count: long,\n",
    "            distinct_count: long,\n",
    "            min_value: long,\n",
    "            max_value: long,\n",
    "            min_length: int,\n",
    "            max_length: int,\n",
    "            total_length: long\n",
    "        >>\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a single row with results for the table\n",
    "    output_row = Row(\n",
    "        table_name=table_name,\n",
    "        layer=layer,\n",
    "        timestamp=timestamp,\n",
    "        proc_date= proc_date,\n",
    "        operation=operation,\n",
    "        deleted_records=int(deleted or 0),\n",
    "        inserted_records=int(inserted or 0),\n",
    "        updated_records=int(updated or 0),\n",
    "        error = None,\n",
    "        status = 'Success',\n",
    "        column_metrics=column_metrics\n",
    "    )\n",
    "    # Return a DataFrame with the specified result schema\n",
    "    return spark.createDataFrame([output_row], result_schema)\n",
    "\n",
    "\n",
    "# Main execution\n",
    "# Initialize empty final DataFrame\n",
    "final_df: DataFrame = None\n",
    "\n",
    "try:\n",
    "    print(f\" Processing table: {table_name}\")\n",
    "    full_table = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "    # Extract 'layer' (bronze/silver/gold) based on schema naming convention\n",
    "    layer = schema_name.split(\"_\")[2] if len(schema_name.split(\"_\")) > 2 else \"unknown\"\n",
    "    # Step 2: Get the latest Delta version history metadata for the table and metrics like numDeletedRows,numOutputRows,numTargetRowsUpdated\n",
    "    \"\"\"\n",
    "    Retrieve the latest Delta version metadata for the table using DESCRIBE HISTORY.\n",
    "    - Constructs a query to extract recent operation info (version, date, operation type).\n",
    "    - Also fetches metrics: deleted, inserted, and updated row counts.\n",
    "    - Returns the most recent entry (latest version) using ROW_NUMBER().\n",
    "    \"\"\"\n",
    "    hist_query = f\"\"\"\n",
    "        SELECT\n",
    "          '{table_name}' AS table_name,\n",
    "          '{layer}' AS layer,\n",
    "          *\n",
    "        except\n",
    "          (rn)\n",
    "        from\n",
    "          (\n",
    "            SELECT\n",
    "              ROW_NUMBER() OVER (\n",
    "                ORDER BY\n",
    "                  version DESC\n",
    "              ) AS rn,\n",
    "              version,\n",
    "              operation,\n",
    "              operationMetrics.numDeletedRows as deleted_records,\n",
    "              operationMetrics.numOutputRows as inserted_records,\n",
    "              operationMetrics.numTargetRowsUpdated as updated_records\n",
    "            FROM(\n",
    "              (DESCRIBE HISTORY {catalog_name}.{schema_name}.{table_name})\n",
    "          )\n",
    "          WHERE operation NOT IN ('VACUUM END', 'VACUUM START', 'OPTIMIZE', 'RESTORE')\n",
    "          )\n",
    "        WHERE\n",
    "          rn = 1\n",
    "        \"\"\"\n",
    "    hist_row = spark.sql(hist_query).first()\n",
    "    # If table has no version history, skip\n",
    "    if not hist_row:\n",
    "        print(f\" No version history found for {full_table}\")\n",
    "        pass\n",
    "    # Extract metadata from latest version row\n",
    "    version = hist_row[\"version\"]\n",
    "    operation = hist_row[\"operation\"]\n",
    "    deleted = hist_row[\"deleted_records\"]\n",
    "    inserted = hist_row[\"inserted_records\"]\n",
    "    updated = hist_row[\"updated_records\"]\n",
    "    # Step 3: Read the table at the specific Delta version\n",
    "    df_table = spark.read.format(\"delta\").option(\"versionAsOf\", version).table(full_table).cache()\n",
    "    # 3. calling compute_column_metrics function\n",
    "    df_profile = compute_column_metrics(\n",
    "        df_table,\n",
    "        table_name=table_name,\n",
    "        layer=layer,\n",
    "        timestamp = timestamp,\n",
    "        proc_date = proc_date,\n",
    "        operation=operation,\n",
    "        deleted=deleted,\n",
    "        inserted=inserted,\n",
    "        updated=updated\n",
    "    )\n",
    "    df_table.unpersist()\n",
    "    # Step 5: Append metrics result to final_df\n",
    "    if df_profile:\n",
    "        final_df = df_profile if final_df is None else final_df.unionByName(df_profile, allowMissingColumns=True)\n",
    "except Exception as e:\n",
    "    print(f\" Error processing {schema_name}.{table_name}: {e}\")\n",
    "\n",
    "# Step 6: Write final metrics result to monitoring_logs table in append mode\n",
    "if final_df is not None:\n",
    "    final_df = (\n",
    "        final_df\n",
    "        .withColumn(\"proc_date\", col(\"proc_date\").cast(\"date\"))\n",
    "        .withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
    "    )\n",
    "    final_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(f\"{catalog_name}.default.monitoring_logs\")\n",
    "    print(\"\\n Monitoring logs written successfully.\")\n",
    "else:\n",
    "    print(\"\\n No metrics data collected.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6350561386724456,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "monitoring_logs_nb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
