{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb0a6576-3c11-4cb8-b5e9-d2b8c6154042",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../utils/common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12f9aaac-e3dc-40e5-844d-9b325f24c5e3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Widget Initialization & Variable Assignment"
    }
   },
   "outputs": [],
   "source": [
    "# Define widgets \n",
    "dbutils.widgets.text(\"environment\", \"\", \"\")\n",
    "dbutils.widgets.text(\"table_key\", \"\", \"\")\n",
    "dbutils.widgets.text(\"job_name\", \"\", \"\")\n",
    "dbutils.widgets.text(\"proc_date\", \"\", \"\")\n",
    "dbutils.widgets.text(\"source_origin\", \"\", \"\")\n",
    "\n",
    "# Retrieve widget values into direct variables\n",
    "environment   = dbutils.widgets.get(\"environment\")\n",
    "table_key     = dbutils.widgets.get(\"table_key\")\n",
    "job_name      = dbutils.widgets.get(\"job_name\")\n",
    "proc_date_str = dbutils.widgets.get(\"proc_date\")\n",
    "source_origin = dbutils.widgets.get(\"source_origin\")\n",
    "\n",
    "# Get catalog from settings config\n",
    "catalog = settings[environment]['catalog_name']\n",
    "\n",
    "print(f\"Environment:   {environment}\")\n",
    "print(f\"Table Key:     {table_key}\")\n",
    "print(f\"Job Name:      {job_name}\")\n",
    "print(f\"Proc Date:     {proc_date_str}\")\n",
    "print(f\"Source Origin: {source_origin}\")\n",
    "print(f\"Catalog:       {catalog}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dac635f-e5d8-4168-8031-ad95c330ead5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Process Date Parsing"
    }
   },
   "outputs": [],
   "source": [
    "# Parse the processing date string into datetime/date\n",
    "try:\n",
    "    proc_date = datetime.strptime(proc_date_str, \"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "except ValueError:\n",
    "    raise ValueError(f\"Invalid date format: {proc_date_str}\")\n",
    "proc_date_val = proc_date.date()\n",
    "dbutils.jobs.taskValues.set(\"proc_date\", proc_date_val.strftime('%Y-%m-%d'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53cede94-918c-4555-a6f1-6bce88d4d4b4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Lookup: Get Source Configuration"
    }
   },
   "outputs": [],
   "source": [
    "# Fetch the file, table, and other configs for given source/table from a lookup table\n",
    "lookup_df = (\n",
    "    spark.table(f\"{catalog}.default.lookup_table_raw_to_silver\")\n",
    "    .filter(\n",
    "        (col(\"source_origin\") == source_origin) & \n",
    "        (col(\"table_key\") == table_key)\n",
    "    )\n",
    ")\n",
    "records = lookup_df.collect()\n",
    "if not records:\n",
    "    raise ValueError(f\"No entry found in lookup table for source_origin = '{source_origin}' and table_key = '{table_key}'\")\n",
    "\n",
    "record = records[0]\n",
    "raw_path_base = record[\"raw_file_path\"].rstrip(\"/\")\n",
    "raw_format = record[\"raw_file_format\"]\n",
    "bronze_schema = record[\"bronze_schema\"]\n",
    "bronze_table_name = record[\"bronze_table_name\"]\n",
    "silver_layer_notebook_path = record[\"silver_layer_notebook_path\"]\n",
    "tbl_bronze = f\"{catalog}.{bronze_schema}.{bronze_table_name}\"\n",
    "\n",
    "print(f\"Raw Path Base: {raw_path_base}\")\n",
    "print(f\"Raw Format: {raw_format}\")\n",
    "print(f\"Bronze Schema: {bronze_schema}\")\n",
    "print(f\"Bronze Table Name: {bronze_table_name}\")\n",
    "print(f\"Silver Layer Notebook Path: {silver_layer_notebook_path}\")\n",
    "print(f\"Target Bronze Table: {tbl_bronze}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f2d0d48-a6c0-47d2-a9cc-3d6a5e80cef3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Build Raw Path (add date partition )"
    }
   },
   "outputs": [],
   "source": [
    "date_suffix = f\"/yyyy={proc_date_val.year}/mm={proc_date_val.strftime('%m')}/dd={proc_date_val.strftime('%d')}\"\n",
    "raw_path = raw_path_base\n",
    "if \"yyyy=\" not in raw_path_base:\n",
    "    raw_path += date_suffix\n",
    "\n",
    "print(f\"Processing Date (date part): {proc_date_val}\")\n",
    "print(f\"Reading from path: {raw_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e80da8af-c9ef-4352-b1dc-d7d9c05dcb6d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Transformation , Source Processing & Silver Processing Functions"
    }
   },
   "outputs": [],
   "source": [
    "# Transformation and Write Function\n",
    "def apply_transformations_and_write(df_raw, tbl_bronze, proc_date_val, write_mode=\"overwrite\", cast_timestamp=False):\n",
    "\n",
    "    for col_name in df_raw.columns:\n",
    "        df_raw = df_raw.withColumnRenamed(col_name, to_snake_case(col_name, source_origin))\n",
    "\n",
    "    if cast_timestamp and 'file_creation_ts' in df_raw.columns:\n",
    "        df_raw = df_raw.withColumn(\"file_creation_ts\", col(\"file_creation_ts\").cast(\"timestamp\"))\n",
    "\n",
    "    df_to_write = df_raw.withColumn(\"proc_date\", to_date(lit(proc_date_val), \"yyyy-MM-dd\"))\n",
    "    if \"file_creation_ts\" not in df_to_write.columns:\n",
    "        df_to_write = df_to_write.withColumn(\"file_creation_ts\", lit(None).cast(\"timestamp\"))\n",
    "\n",
    "    (\n",
    "        df_to_write.write\n",
    "        .mode(write_mode)\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .option(\"replaceWhere\", f\"proc_date = '{proc_date_val}'\")\n",
    "        .format(\"delta\")\n",
    "        .saveAsTable(tbl_bronze)\n",
    "    )\n",
    "\n",
    "# Source Processing Block\n",
    "def process_all_sources(source_origin, spark, dbutils, raw_path, raw_format, tbl_bronze, proc_date_val, table_key=None):\n",
    "    print(f\"Executing logic for {source_origin}\")\n",
    "\n",
    "    if source_origin == \"SAP BW WCM\":\n",
    "        spark.sql(f\"DELETE FROM {tbl_bronze} WHERE proc_date = '{proc_date_val}'\")\n",
    "        parquet_file_list = [file.path for file in dbutils.fs.ls(raw_path) if file.path.endswith('.parquet')]\n",
    "        print(f\"Found {len(parquet_file_list)} Parquet files.\")\n",
    "        \n",
    "        for file_path in parquet_file_list:\n",
    "            df_raw = spark.read.format(raw_format).load(file_path)\n",
    "            n_records = df_raw.count()\n",
    "            if n_records > 0 and 'file_creation_ts' in df_raw.columns:\n",
    "                print(f\"Ingesting {n_records} records from {file_path}\")\n",
    "                apply_transformations_and_write(df_raw, tbl_bronze, proc_date_val, write_mode=\"append\", cast_timestamp=True)\n",
    "                print(f\"Success: Loaded data from {file_path}\")\n",
    "            else:\n",
    "                print(f\"No data found in file {file_path}\")\n",
    "\n",
    "    elif source_origin == \"ROP\" and table_key:\n",
    "        df_raw = spark.read.format(raw_format).load(raw_path)\n",
    "        schema_mapping = {\n",
    "            \"rop_order_report\": [\n",
    "                \"serial_number\", \"order_id\", \"region\", \"city\", \"vendor_id\", \"vendor_name\", \"vendor_address\",\n",
    "                \"sub_range_id\", \"sub_range_name\", \"store_id\", \"store_name\", \"store_address\", \"order_user\",\n",
    "                \"product_stt\", \"product_id\", \"product_name\", \"barcode\", \"baseuom\", \"price\",\n",
    "                \"purchase_order_quantity\", \"purchase_order_quantity_delivery\",\n",
    "                \"purchase_order_quantity_delivered_actual\", \"order_date\", \"order_date_request\",\n",
    "                \"order_date_expected_delivery\", \"time_frame_of_delivery\", \"delivery_confirmation_date\",\n",
    "                \"time_frame_for_delivery_confirmation\", \"delivery_date\", \"status\", \"upd_date\", \"note_1\", \"note_2\"\n",
    "            ],\n",
    "            \"rop_problem_article\": [\"index\", \"domain\", \"area\", \"sku\", \"reason\", \"user_updated\", \"updated_at\", \"active_status\"]\n",
    "        }\n",
    "        if table_key in schema_mapping:\n",
    "            df_raw = df_raw.toDF(*schema_mapping[table_key])\n",
    "        n_records = df_raw.count()\n",
    "        if n_records > 0:\n",
    "            print(f\"Ingesting {n_records} records\")\n",
    "            apply_transformations_and_write(df_raw, tbl_bronze, proc_date_val)\n",
    "            print(f\"Success: Loaded data into {tbl_bronze}\")\n",
    "        else:\n",
    "            print(f\"No data found for date: {proc_date_val}\")\n",
    "\n",
    "    else:\n",
    "        df_raw = spark.read.format(raw_format).load(raw_path)\n",
    "        n_records = df_raw.count()\n",
    "        if n_records > 0:\n",
    "            print(f\"Ingesting {n_records} records\")\n",
    "            apply_transformations_and_write(df_raw, tbl_bronze, proc_date_val)\n",
    "            print(f\"Success: Loaded data into {tbl_bronze}\")\n",
    "        else:\n",
    "            print(f\"No data found for date: {proc_date_val}\")\n",
    "\n",
    "# Silver Layer Notebook Execution Block\n",
    "def execute_silver_layer_notebook(dbutils, silver_layer_notebook_path, tbl_bronze, proc_date_val, environment, bronze_table_name):\n",
    "    if not silver_layer_notebook_path.strip():\n",
    "        raise ValueError(f\"Silver notebook path for bronze table {tbl_bronze} does not exist.\")\n",
    "    print(f\"Triggering Silver Layer Notebook: {silver_layer_notebook_path}\")\n",
    "    dbutils.notebook.run(\n",
    "        silver_layer_notebook_path,\n",
    "        timeout_seconds=0,\n",
    "        arguments={\n",
    "            \"proc_date\": proc_date_val.strftime('%Y-%m-%d'),\n",
    "            \"environment\": environment,\n",
    "            \"table\": bronze_table_name,\n",
    "            \"layer\": \"silver\"\n",
    "        }\n",
    "    )\n",
    "    print(f\"Completed Silver Layer Notebook: {silver_layer_notebook_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea6ff577-3d43-4d94-96af-cfef686535ca",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Supported Source Check & Orchestration"
    }
   },
   "outputs": [],
   "source": [
    "SUPPORTED_SOURCES = [\n",
    "    \"SAP BW WCM\", \"ROP\", \"CAPILLARY\", \"CX LOYALTY\", \"DATA PORTAL\", \"SAP CAR\", \"SAP ERP WCM\",\"SUPRA\"\n",
    "]\n",
    "if source_origin not in SUPPORTED_SOURCES:\n",
    "    raise ValueError(f\"Processing logic is not defined for source_origin: '{source_origin}'\")\n",
    "\n",
    "# Main execution\n",
    "process_all_sources(\n",
    "    source_origin=source_origin,\n",
    "    spark=spark,\n",
    "    dbutils=dbutils,\n",
    "    raw_path=raw_path,\n",
    "    raw_format=raw_format,\n",
    "    tbl_bronze=tbl_bronze,\n",
    "    proc_date_val=proc_date_val,\n",
    "    table_key=table_key\n",
    ")\n",
    "\n",
    "\n",
    "execute_silver_layer_notebook(\n",
    "    dbutils=dbutils,\n",
    "    silver_layer_notebook_path=silver_layer_notebook_path,\n",
    "    tbl_bronze=tbl_bronze,\n",
    "    proc_date_val=proc_date_val,\n",
    "    environment=environment,\n",
    "    bronze_table_name=bronze_table_name\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6403329334323067,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "raw_to_silver",
   "widgets": {
    "environment": {
     "currentValue": "DEV",
     "nuid": "072e683c-1415-48ce-b04c-227ad2816450",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "environment",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "environment",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "job_name": {
     "currentValue": "",
     "nuid": "05a00deb-0172-43fa-97f1-d8477a7c7109",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "job_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "job_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "proc_date": {
     "currentValue": "2025-07-15T15:11:38.345",
     "nuid": "3e29ca29-7df3-4b6d-a3b7-52e36d27ff5c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "proc_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "proc_date",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "source_origin": {
     "currentValue": "DATA PORTAL",
     "nuid": "4b0c4c9e-3944-4b09-9c3f-f13b3a9f7f47",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "source_origin",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "source_origin",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "table_key": {
     "currentValue": "dataportal_fnr_fresh_order_config_delivery_schedule_v2",
     "nuid": "23a5cc40-81d6-409f-abed-0c1c91a5e16c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "table_key",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "table_key",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
