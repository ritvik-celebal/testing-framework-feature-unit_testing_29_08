{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "049bcd06-be92-4f86-8961-5a142d92743f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../../utils/common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2989d502-a6b2-41cf-b8c5-4904e9ca285e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"proc_date\", \"\")\n",
    "proc_date = dbutils.widgets.get(\"proc_date\")\n",
    "\n",
    "dbutils.widgets.text(\"environment\", \"\")\n",
    "environment = dbutils.widgets.get(\"environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd866ae0-0b88-4552-ad74-bf09198cc3dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog_name = settings[environment]['catalog_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21c3a160-d04a-46d3-9dd7-2f802c4e7a66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import re\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"SnakeCaseView\").getOrCreate()\n",
    "\n",
    "# Define your bronze table and temp view name\n",
    "bronze_table = \"udp_wcm_pro.udp_wcm_bronze_pro.supra_f_productivity\"\n",
    "temp_view = \"temp_supra_f_productivity\"\n",
    "\n",
    "# List of column names\n",
    "column_names = [\n",
    "    \"ClientCode\", \"Employee\", \"EmployeeName\", \"EndTime\", \"IsEven\", \"JobType\",\n",
    "    \"ObjectCode\", \"ProcDate\", \"RefCode\", \"StartTime\", \"TotalSKU\",\n",
    "    \"TotalSKUProcessed\", \"TotalUnit\", \"TotalUnitProcessed\", \"WarehouseCode\",\n",
    "    \"WarehouseSiteId\"\n",
    "]\n",
    "\n",
    "# Function to convert camel case to snake case\n",
    "def to_snake_case(name):\n",
    "    name = name.replace('/', '_')  # Replace any existing slashes with underscores\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)  # Add underscore before uppercase letters\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()  # Final conversion to snake case\n",
    "\n",
    "# Build the dynamic SELECT query with renamed columns\n",
    "dynamic_query = f\"CREATE OR REPLACE TEMP VIEW {temp_view} AS SELECT \"\n",
    "\n",
    "for col in column_names:\n",
    "    snake_col = to_snake_case(col)\n",
    "    dynamic_query += f\"`{col}` AS {snake_col}, \"\n",
    "\n",
    "# Remove trailing comma and add FROM clause with filtering\n",
    "dynamic_query = dynamic_query.rstrip(', ') + f\"\"\"\n",
    "FROM {bronze_table}\n",
    "WHERE to_date(ProcDate, 'yyyyMMddHHmmss') = DATE('{proc_date}')\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query to create the view\n",
    "spark.sql(dynamic_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ba02401-a5aa-4a4e-9483-95eccff0e420",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {catalog_name}.udp_wcm_silver_realtime.sft_productivity  \n",
    " ( hash_id BIGINT,\n",
    "  client_code STRING,\n",
    "    employee_id STRING,\n",
    "    employee_name STRING,\n",
    "    end_time STRING,\n",
    "    is_even BOOLEAN,\n",
    "    job_type STRING,\n",
    "    object_code STRING,\n",
    "    proc_date DATE,\n",
    "    ref_code STRING,\n",
    "    start_time STRING,\n",
    "    total_sku BIGINT,\n",
    "    total_sku_processed BIGINT,\n",
    "    total_unit BIGINT,\n",
    "    total_unit_processed DOUBLE,\n",
    "    warehouse_code STRING,\n",
    "    warehouse_site_id STRING\n",
    ")TBLPROPERTIES (\n",
    "  'DELTA.AUTOOPTIMIZE.OPTIMIZEWRITE' = 'TRUE',\n",
    "  'DELTA.AUTOOPTIMIZE.AUTOCOMPACT' = 'TRUE'\n",
    ")\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af742e66-c6e1-4ba9-8285-11a0e44ce262",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW supra_f_productivity AS\n",
    "SELECT\n",
    "  farm_fingerprint(CONCAT(\n",
    "      IFNULL(CAST(client_code AS STRING), \"\"),\n",
    "      IFNULL(CAST(warehouse_code AS STRING), \"\"),\n",
    "      IFNULL(CAST(warehouse_site_id AS STRING), \"\"),\n",
    "      IFNULL(CAST(object_code AS STRING), \"\"),\n",
    "      IFNULL(CAST(ref_code AS STRING), \"\"),\n",
    "      IFNULL(CAST(job_type AS STRING), \"\"),\n",
    "      IFNULL(CAST(is_even AS STRING), \"\"),\n",
    "      IFNULL(CAST(employee AS STRING), \"\"),\n",
    "      IFNULL(CAST(start_time AS STRING), \"\"),\n",
    "      IFNULL(CAST(end_time AS STRING), \"\"),\n",
    "      IFNULL(CAST(proc_date AS STRING), \"\")\n",
    "  )) AS hash_id,\n",
    "  client_code,\n",
    "  warehouse_code,\n",
    "  warehouse_site_id,\n",
    "  object_code,\n",
    "  ref_code,\n",
    "  job_type,\n",
    "  is_even,\n",
    "  employee AS employee_id,\n",
    "  employee_name,\n",
    "  TO_TIMESTAMP(start_time, 'yyyy-MM-dd HH:mm:ss') AS start_time,\n",
    "  TO_TIMESTAMP(end_time, 'yyyy-MM-dd HH:mm:ss') AS end_time,\n",
    "  total_sku,\n",
    "  total_sku_processed,\n",
    "  total_unit,\n",
    "  total_unit_processed,\n",
    "  TO_TIMESTAMP(proc_date, 'yyyyMMddHHmmss') AS proc_date\n",
    "FROM temp_supra_f_productivity;\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2ffc8fb-e04f-4415-b395-dd8edef3152a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "MERGE INTO {catalog_name}.udp_wcm_silver_realtime.sft_productivity AS main_tbl\n",
    "USING supra_f_productivity AS temp_tbl \n",
    "ON main_tbl.hash_id = temp_tbl.hash_id\n",
    "WHEN NOT MATCHED THEN \n",
    "  INSERT (hash_id, client_code, warehouse_code, warehouse_site_id, object_code, ref_code, job_type, is_even, employee_id, employee_name, start_time, end_time, total_sku, total_sku_processed, total_unit, total_unit_processed, proc_date)\n",
    "  VALUES (temp_tbl.hash_id, temp_tbl.client_code, temp_tbl.warehouse_code, temp_tbl.warehouse_site_id, temp_tbl.object_code, temp_tbl.ref_code, temp_tbl.job_type, temp_tbl.is_even, temp_tbl.employee_id, temp_tbl.employee_name, temp_tbl.start_time, temp_tbl.end_time, temp_tbl.total_sku, temp_tbl.total_sku_processed, temp_tbl.total_unit, temp_tbl.total_unit_processed, temp_tbl.proc_date)\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "sft_productivity",
   "widgets": {
    "environment": {
     "currentValue": "DEV",
     "nuid": "438299c4-1d2f-486c-93c4-7b86e44954e1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "environment",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "environment",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "proc_date": {
     "currentValue": "2025-05-31",
     "nuid": "f177abe2-0006-4813-a810-4d5cbfaa14e1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "proc_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "proc_date",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
